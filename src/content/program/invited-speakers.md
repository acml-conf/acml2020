---
content_id: program-invited-speakers
title: Invited Speakers
path: /program/invited-speakers
withSectionMenu: true
---

## Suriya Gunasekar
<div align="center">
    <img width="200px" src="pathPrefix::/invited-speakers/SuriyaGunasekar.jpg"/>
</div>

**Affiliation:** Microsoft Research, USA

**Title:** TBA

**Abstract:**

TBA

**Bio:**

Suriya Gunasekar is a Senior Researcher in the Machine Learning and Optimization (MLO) group at Microsoft Research at Redmond. Prior to joining MSR, she was a Research Assistant Professor at Toyota Technological Institute at Chicago. She received her PhD in Electrical and Computer Engineering from The University of Texas at Austin. 

## Alice Oh
<div align="center">
    <img width="200px" src="pathPrefix::/invited-speakers/AliceOh.jpg"/>
</div>

**Affiliation:** KAIST, Korea

**Title:** Open domain dialogue response generation: models and evaluation metrics

**Abstract:**

Dialogue response generation is an important AI research topic. We are investigating this topic with the open domain perspective, meaning the topics of the dialogues are not pre-specified. I describe a new model called Variable Hierarchical User-based Conversation Model which considers the previous dialogues among users, as well as their social network connections. I then describe a new evaluation metric called Speaker Sensitive Response Evaluation Model which correlates better with human judgments. These are described in our recent EMNLP 2019 and ACL 2020 papers.

**Bio:**

Alice Oh is an Associate Professor in the School of Computing at KAIST and directs the Users and Information Lab. She received MS in Language and Information Technologies from CMU and PhD in Computer Science from MIT. Her main research interests are in developing ML techniques for NLP, applying them to human social behavior data, and improving CS education with ML. She serves the ML and NLP research communities by reviewing and chairing for conferences including ACL, EMNLP, ACML, ICML, NeurIPS, and ICLR. Her most recent service is co-Program Chair of ICLR 2021.

## Lee Wee Sun
<div align="center">
    <img width="200px" src="pathPrefix::/invited-speakers/WeeSunLee.jpg"/>
</div>

**Affiliation:** National University of Singapore, Singapore

**Title:** Neuralizing Algorithms

**Abstract:**

TBA

**Bio:**

Lee Wee Sun is a professor in the Department of Computer Science, National University of Singapore. He obtained his B.Eng from the University of Queensland in 1992 and his Ph.D. from the Australian National University in 1996. He has been a research fellow at the Australian Defence Force Academy, a fellow of the Singapore-MIT Alliance, and a visiting scientist at MIT.  

His research interests include machine learning, planning under uncertainty, and approximate inference. He has been an area chair for machine learning and AI conferences such as the Neural Information Processing Systems (NeurIPS), the International Conference on Machine Learning (ICML), the AAAI Conference on Artificial Intelligence (AAAI), and the International Joint Conference on Artificial Intelligence (IJCAI). He was a program, conference and journal track co-chair for the Asian Conference on Machine Learning (ACML), and he is currently the co-chair of the steering committee of ACML.


## Chidchanok Lursinsap
<div align="center">
    <img width="200px" src="pathPrefix::/invited-speakers/chidchnok.jpg"/>
</div>

**Affiliation:** Chulalongkorn University, Bangkok, Thailand

**Title:** Fast and Accurate Neural Learning with Limited Memory Size, Limited Energy Supply, and Class Drift Constraints in Streaming Data Environment

**Abstract:**

Tremendous data have been generated in almost every field of industrial and scientific applications and researches Due to the advancement of Internet and new sensor equipment. This situation creates a crisis of memory overflow, where the amount of continuously incoming data is larger than the physical size of memory. Most of the developed neural learning algorithms were designed without seriously considering this memory overflow crisis. It is assumed that all learning data including present data and new incoming data must be retained inside the memory throughout the learning process. This assumption is unrealistic and impractical in the streaming data environment. Furthermore, the number of learning epochs cannot be controlled, which implies that the energy consumption for achieving the learning process may exceed the available energy supply such as a battery. This talk will discuss a new concept of neural learning, the supporting architecture, and the relevant theoretical foundation to achieve the efficient leaning process with high accuracy under the constraints of memory overflow and controllable polynomial time complexity.

**Bio:**

Chidchanok Lursinsap received the B.Eng. degree (Hons.) in computer engineering from Chulalongkorn University, Bangkok,
Thailand, in 1978, and the M.S. and Ph.D. degrees in computer science from the University of Illinois at
Urbana–Champaign, Urbana, IL, USA, in 1982 and 1986, respectively.
He was a Lecturer with the Department of Computer Engineering, Chulalongkorn University, in 1979. In 1986,
he was a Visiting Assistant Professor with the Department of Computer Science, University of Illinois at Urbana–Champaign.
From 1987 to 1996, he was with the Center for Advanced Computer Studies, University of Louisiana at Lafayette,
as an Assistant and an Associate Professor.
After that, he came back to Thailand to establish the Ph.D. Program in computer science with Chulalongkorn University
and he became a Full Professor. His major research interests include neural learning and its applications to other science and engineering areas.